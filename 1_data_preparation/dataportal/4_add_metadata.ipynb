{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.436340Z",
     "start_time": "2020-11-04T12:31:48.732042Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "node = !hostname\n",
    "if \"sc\" in node[0]:\n",
    "    base_path = \"/sc-projects/sc-proj-ukb-cvd\"\n",
    "else: base_path = \"/data/analysis/ag-reils/ag-reils-shared/cardioRS\"\n",
    "print(base_path)\n",
    "\n",
    "dataset_name = \"211110_anewbeginning\"\n",
    "mapping_path = f\"{base_path}/data/mapping\"\n",
    "dataset_path = f\"{base_path}/data/2_datasets_pre/{dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.895222Z",
     "start_time": "2020-11-04T12:31:49.891332Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(dataset_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "project=\"recordgraphs\"\n",
    "entity=\"cardiors\"\n",
    "artifact_date = \"220621\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_artifact(project, entity, artifact_name, type_name, description, artifact_path, df):\n",
    "    \n",
    "    run = wandb.init(project=project, job_type=\"log_artifact\", entity=entity, tags=[\"artifact\"])\n",
    "    \n",
    "    artifact = wandb.Artifact(artifact_name, type=type_name, \n",
    "                              description=description,\n",
    "                             metadata = {\"1_shape\": f\"{len(df)}x{len(df.columns)}\",\n",
    "                                         \"2_cols\": str(df.columns.to_list())})\n",
    "    artifact.add_reference(f\"\"\"file://{artifact_path}\"\"\", artifact_name, checksum=True)\n",
    "    run.log_artifact(artifact)\n",
    "    \n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare Patient Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mappings + Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.867152Z",
     "start_time": "2020-11-04T12:33:16.878773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_dir = f\"{mapping_path}/athena\"\n",
    "vocab = {\n",
    "    \"concept\": pd.read_csv(f\"{vocab_dir}/CONCEPT.csv\", sep='\\t'),\n",
    "    \"domain\": pd.read_csv(f\"{vocab_dir}/DOMAIN.csv\", sep='\\t'),\n",
    "    \"class\": pd.read_csv(f\"{vocab_dir}/CONCEPT_CLASS.csv\", sep='\\t'),\n",
    "    \"relationship\": pd.read_csv(f\"{vocab_dir}/RELATIONSHIP.csv\", sep='\\t'),\n",
    "    \"drug_strength\": pd.read_csv(f\"{vocab_dir}/DRUG_STRENGTH.csv\", sep='\\t'),\n",
    "    \"vocabulary\": pd.read_csv(f\"{vocab_dir}/VOCABULARY.csv\", sep='\\t'),\n",
    "    \"concept_synonym\": pd.read_csv(f\"{vocab_dir}/CONCEPT_SYNONYM.csv\", sep='\\t'),\n",
    "    \"concept_ancestor\": pd.read_csv(f\"{vocab_dir}/CONCEPT_ANCESTOR.csv\", sep='\\t'),\n",
    "    \"concept_relationship\": pd.read_csv(f\"{vocab_dir}/CONCEPT_RELATIONSHIP.csv\", sep='\\t')                       \n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.872216Z",
     "start_time": "2020-11-04T12:34:05.869505Z"
    }
   },
   "source": [
    "#time0_col=\"birth_date\"\n",
    "time0_col=\"date_of_attending_assessment_centre_f53_0_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.889725Z",
     "start_time": "2020-11-04T12:34:05.874587Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_fields(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields) & data_field[\"field.tab\"].str.contains(\"f\\\\.\\\\d+\\\\.0\\\\.\\\\d\")].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_fields_all(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields)].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_data_fields(fields, data, data_field):\n",
    "    f = get_fields(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()\n",
    "\n",
    "def get_data_fields_all(fields, data, data_field):\n",
    "    f = get_fields_all(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coding10 = pd.read_csv(f\"{mapping_path}/codings/coding10.tsv\", sep=\"\\t\").assign(coding = lambda x: x.coding.astype(\"int\")).rename(columns={\"coding\":\"uk_biobank_assessment_centre_f54_0_0\"})\n",
    "coding10[\"uk_biobank_assessment_centre_f54_0_0\"] = coding10[\"uk_biobank_assessment_centre_f54_0_0\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:33:14.171198Z",
     "start_time": "2020-11-04T12:31:50.204540Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_feather(f\"{base_path}/data/1_decoded/ukb_data.feather\")\n",
    "data_field = pd.read_feather(f\"{base_path}/data/1_decoded/ukb_data_field.feather\")\n",
    "data_columns = data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop obviouse missing data\n",
    "print(len(data))\n",
    "data = data.dropna(subset=[\"sex_f31_0_0\"], axis=0)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fields_basics = [\n",
    "    \"21022\", # age at recruitment\n",
    "    \"31\", # sex\n",
    "    \"21000\", # ethnicity\n",
    "   # \"189\", # Townsend index\n",
    "    \"53\", # date of baseline assessment\n",
    "    \"54\", # assessment center\n",
    "]\n",
    "\n",
    "temp = get_data_fields(fields_basics, data, data_field)\n",
    "\n",
    "temp[\"sex_f31_0_0\"] = temp[\"sex_f31_0_0\"].cat.set_categories([\"Female\", 'Male'], ordered=False)\n",
    "\n",
    "#temp[\"ethnic_background_f21000_0_0\"] = temp[\"ethnic_background_f21000_0_0\"].astype(\"string\")\n",
    "\n",
    "ethn_bg_def = {#\"White\": [\"White\", \"British\", \"Irish\", \"Any other white background\"],\n",
    "#                \"Mixed\": [\"Mixed\", \"White and Black Caribbean\", \"White and Black African\", \"White and Asian\", \"Any other mixed background\"],  \n",
    "##                \"Asian\": [\"Asian or Asian British\", \"Indian\", \"Pakistani\", \"Bangladeshi\", \"Any other Asian background\"], \n",
    "#                \"Black\": [\"Black or Black British\", \"Caribbean\", \"African\", \"Any other Black background\"],\n",
    "#                \"Chinese\": [\"Chinese\"],  \n",
    "                np.nan: [\"Other ethnic group\", \"Do not know\", \"Prefer not to answer\"]}\n",
    "\n",
    "ethn_bg_dict = {}\n",
    "for key, values in ethn_bg_def.items(): \n",
    "    for value in values:\n",
    "        ethn_bg_dict[value]=key \n",
    "        \n",
    "temp[\"ethnic_background_f21000_0_0\"].replace(ethn_bg_dict, inplace=True)\n",
    "temp[\"ethnic_background_f21000_0_0\"] = temp[\"ethnic_background_f21000_0_0\"].astype(\"category\")\n",
    "\n",
    "#\n",
    "#temp[\"ethnic_background_f21000_0_0\"] = temp[\"ethnic_background_f21000_0_0\"].astype(\"category\").cat.set_categories(['White', 'Black', 'Asien', 'Mixed', 'Chinese'], ordered=False)\n",
    "\n",
    "basics = temp\n",
    "print(len(temp))\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "calc_birth_date = [date_of_attending_assessment_centre - relativedelta(years=age_at_recruitment) \n",
    "                                                             for date_of_attending_assessment_centre, age_at_recruitment \n",
    "                                                             in zip(basics[\"date_of_attending_assessment_centre_f53_0_0\"], basics[\"age_at_recruitment_f21022_0_0\"])]\n",
    "\n",
    "basics = basics.assign(birth_date = calc_birth_date)\n",
    "basics[\"uk_biobank_assessment_centre_f54_0_0\"] = basics.assign(uk_biobank_assessment_centre_f54_0_0 = lambda x: x.uk_biobank_assessment_centre_f54_0_0.astype(\"int\")).merge(coding10, on=\"uk_biobank_assessment_centre_f54_0_0\")[\"meaning\"]\n",
    "\n",
    "\n",
    "display(basics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artifact_name = \"baseline_covariates\"\n",
    "type_name = \"prepare_covariates\"\n",
    "description = \"Dataframe of covariates at recruitment\"\n",
    "artifact_path = f\"{dataset_path}/{artifact_name}_{artifact_date}.feather\"\n",
    "basics.to_feather(artifact_path)\n",
    "create_artifact(project, entity, artifact_name, type_name, description, artifact_path, basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load complete data from GP and HES and ONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Format should be similar as example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = '/sc-projects/sc-proj-ukb-cvd/data/1_decoded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_records = pd.read_feather(f\"{out_path}/dataportal_records_omop_220407.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_records.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artifact_name = \"complete_records\"\n",
    "type_name = \"prepare_records\"\n",
    "description = \"Complete records from dataportal in long format\"\n",
    "artifact_path = f\"{dataset_path}/{artifact_name}_{artifact_date}.feather\"\n",
    "patient_records.to_feather(artifact_path)\n",
    "create_artifact(project, entity, artifact_name, type_name, description, artifact_path, patient_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Long Records Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import wandb\n",
    "\n",
    "def get_path_from_wandb(reference: str):\n",
    "    path = pathlib.Path(reference.split(\"file://\")[1])\n",
    "    assert path.exists()\n",
    "    return path\n",
    "\n",
    "def read_artifact(run, type_name, artifact_name):\n",
    "    \n",
    "    artifact = run.use_artifact(f'cardiors/recordgraphs/{artifact_name}:latest', type=type_name)\n",
    "    file_path = get_path_from_wandb(artifact.manifest.entries[artifact_name].ref)\n",
    "    print(file_path)\n",
    "\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=project, job_type=\"log_artifact\", entity=entity, tags=[\"artifacts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basics = pd.read_feather(read_artifact(run, \"prepare_covariates\", \"baseline_covariates\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complete_records = pd.read_feather(read_artifact(run, \"prepare_records\", \"complete_records\"))\\\n",
    "    .assign(concept_id = lambda x: x.concept_id.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sensible exit date\n",
    "complete_records.groupby(\"origin\")[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "deaths = complete_records.query(\"origin=='death_records'\")[[\"eid\", \"date\"]].drop_duplicates().rename(columns={\"date\":\"death_date\"})\n",
    "extended = basics.merge(deaths, on=\"eid\", how=\"left\")[[\"eid\", \"birth_date\", \"date_of_attending_assessment_centre_f53_0_0\", \"death_date\"]].set_index(\"eid\").rename(columns={\"date_of_attending_assessment_centre_f53_0_0\":\"recruitment_date\"})\n",
    "extended = extended.where(extended.notnull(), pd.NaT).assign(cens_date=datetime.date(2021, 9, 24))\n",
    "extended[\"exit_date\"] = np.minimum(extended[\"death_date\"].values, extended[\"cens_date\"].values)\n",
    "#extended.reset_index().to_feather(os.path.join(dataset_path, 'temp_extended.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extended = extended.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_artifact(run, artifact_name, type_name, description, artifact_path, df):\n",
    "   \n",
    "    artifact = wandb.Artifact(artifact_name, type=type_name, \n",
    "                              description=description,\n",
    "                             metadata = {\"1_shape\": f\"{len(df)}x{len(df.columns)}\",\n",
    "                                         \"2_cols\": str(df.columns.to_list())})\n",
    "    artifact.add_reference(f\"\"\"file://{artifact_path}\"\"\", artifact_name, checksum=True)\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# log in wandb\n",
    "artifact_name = \"metadata_individuals\"\n",
    "type_name = \"prepare_records\"\n",
    "description = \"Metadata for individuals\"\n",
    "artifact_path = f\"{dataset_path}/artifacts/{artifact_name}_{artifact_date}.feather\"\n",
    "extended.to_feather(artifact_path)\n",
    "add_artifact(run, artifact_name, type_name, description, artifact_path, extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_matrix = complete_records\\\n",
    "    .sort_values([\"eid\", \"date\", \"origin\", \"domain_id\", \"code\", \"concept_id\"])\\\n",
    "    .reset_index(drop=True).set_index(\"eid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_matrix_long = extended.set_index(\"eid\")\\\n",
    "    .merge(records_matrix, left_index=True, right_index=True, how=\"left\")\\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_matrix_long = records_matrix_long.query(\"concept_id==concept_id\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep origin column\n",
    "records_matrix_long.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_artifact(run, artifact_name, type_name, description, artifact_path, df):\n",
    "   \n",
    "    artifact = wandb.Artifact(artifact_name, type=type_name, \n",
    "                              description=description,\n",
    "                             metadata = {\"1_shape\": f\"{len(df)}x{len(df.columns)}\",\n",
    "                                         \"2_cols\": str(df.columns.to_list())})\n",
    "    artifact.add_reference(f\"\"\"file://{artifact_path}\"\"\", artifact_name, checksum=True)\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# log in wandb\n",
    "artifact_name = \"complete_records_extended\"\n",
    "type_name = \"prepare_records\"\n",
    "description = \"First patient records from the data portal in long format\"\n",
    "artifact_path = f\"{dataset_path}/{artifact_name}_{artifact_date}.feather\"\n",
    "records_matrix_long.to_feather(artifact_path)\n",
    "add_artifact(run, artifact_name, type_name, description, artifact_path, records_matrix_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ehrgraphs]",
   "language": "python",
   "name": "conda-env-ehrgraphs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}